---
phase: 04-intelligent-scroll-control
plan: 04
type: execute
wave: 3
depends_on: ["04-01", "04-02", "04-03"]
files_modified:
  - script.js
autonomous: false

must_haves:
  truths:
    - "Voice transcripts flow through confidence-aware matching"
    - "AudioVisualizer shows confidence level visually"
    - "Scroll behavior responds to confidence state"
    - "User sees smooth off-script/on-script transitions"
  artifacts:
    - path: "script.js"
      provides: "Full integration of confidence system"
      contains: ["getMatchWithConfidence", "setConfidenceLevel", "updateConfidence"]
  key_links:
    - from: "script.js"
      to: "matching/TextMatcher.js"
      via: "getMatchWithConfidence call"
      pattern: "textMatcher\\.getMatchWithConfidence"
    - from: "script.js"
      to: "voice/AudioVisualizer.js"
      via: "setConfidenceLevel call"
      pattern: "audioVisualizer\\.setConfidenceLevel"
    - from: "script.js"
      to: "matching/ScrollSync.js"
      via: "updateConfidence call"
      pattern: "scrollSync\\.updateConfidence"
---

<objective>
Wire confidence system through script.js and verify intelligent scroll behavior

Purpose: Connect all Phase 4 components - when voice transcripts arrive, use confidence-aware matching to update the scroll state machine and visual indicator. This completes the "teleprompter that follows you" experience.

Output: Integrated script.js that passes confidence through the entire system, plus human verification of the complete behavior
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-intelligent-scroll-control/04-CONTEXT.md
@.planning/phases/04-intelligent-scroll-control/04-RESEARCH.md
@.planning/phases/04-intelligent-scroll-control/04-01-SUMMARY.md
@.planning/phases/04-intelligent-scroll-control/04-02-SUMMARY.md
@.planning/phases/04-intelligent-scroll-control/04-03-SUMMARY.md
@script.js
@matching/TextMatcher.js
@matching/ScrollSync.js
@voice/AudioVisualizer.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integrate confidence system in script.js</name>
  <files>script.js</files>
  <action>
Update script.js to wire the confidence-aware system together:

1. **Update the onTranscript callback in enableVoiceMode():**
   Replace the existing transcript handling (around line 251-271) with confidence-aware version:

   ```javascript
   onTranscript: (text, isFinal) => {
     console.log(`[Voice] ${isFinal ? 'FINAL' : 'interim'}: ${text}`);

     if (textMatcher && scrollSync) {
       // Use confidence-aware matching
       const result = textMatcher.getMatchWithConfidence(text);

       // Update scroll state machine
       const newState = scrollSync.updateConfidence(result);

       // Update visual confidence indicator
       if (audioVisualizer) {
         audioVisualizer.setConfidenceLevel(result.level);
       }

       // Update highlight if we have a position
       if (result.position !== null && highlighter) {
         highlighter.highlightPosition(result.position, textMatcher.scriptWords);
       }

       // Debug logging
       console.log(`[Matching] Position: ${result.position}, Confidence: ${result.level} (${(result.confidence * 100).toFixed(0)}%), State: ${newState}`);
     }
   },
   ```

2. **Update initMatchingSystem() to pass callbacks to ScrollSync:**
   When creating scrollSync, add callbacks for state changes:

   ```javascript
   scrollSync = new ScrollSync(teleprompterContainer, teleprompterText, {
     baseSpeed: 60,
     onStateChange: (newState, prevState) => {
       console.log(`[Scroll] State: ${prevState} -> ${newState}`);
     },
     onConfidenceChange: (level, confidence) => {
       // Confidence change is handled in onTranscript already
     }
   });
   ```

3. **Import ScrollState for state comparison (optional but helpful):**
   At the top of initMatchingSystem or in the dynamic import section:
   ```javascript
   const { ScrollSync, ScrollState } = scrollModule;
   ```
   Store ScrollState in a module-level variable if needed for UI logic.

4. **Clean up: Remove the old manual scroll-based logic when voice is active**
   The voice mode should now fully delegate scroll control to ScrollSync's state machine. The manual play/pause still works but doesn't interfere with voice-controlled scrolling.

5. **Reset confidence state when exiting teleprompter mode:**
   In switchMode('editor'), after calling scrollSync.reset():
   ```javascript
   if (audioVisualizer) {
     audioVisualizer.setConfidenceLevel('high'); // Reset to default
   }
   ```

6. **Ensure setTotalWords is called:**
   In initMatchingSystem, after creating textMatcher:
   ```javascript
   // Set total words on scrollSync for boundary calculations
   if (scrollSync && textMatcher) {
     scrollSync.totalWords = textMatcher.scriptWords.length;
   }
   ```
  </action>
  <verify>
In script.js:
- onTranscript calls textMatcher.getMatchWithConfidence()
- scrollSync.updateConfidence() is called with match result
- audioVisualizer.setConfidenceLevel() is called with result.level
- Console logs show confidence level and scroll state
- scrollSync.totalWords is set during initialization
  </verify>
  <done>
Voice transcripts flow through confidence-aware matching, scroll state machine updates, and visual indicator reflects confidence
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete intelligent scroll control system:
- Confidence calculation from match quality, consecutive matches, and recency
- Three-state scroll machine (CONFIDENT/UNCERTAIN/OFF_SCRIPT)
- Exponential easing for smooth speed transitions
- Visual confidence indicator (waveform brightness)
- Skip detection with confidence thresholds
- Boundary enforcement (never scrolls past last spoken text)
- Removed dimming of read text
  </what-built>
  <how-to-verify>
1. Open the app (serve it locally if needed: `npx serve .` or `python -m http.server`)
2. Paste a medium-length script (several paragraphs)
3. Click "Start" to enter teleprompter mode
4. Enable voice mode (microphone icon)
5. Test each scenario:

**Test A: Normal reading**
- Read the script naturally
- EXPECTED: Waveform is bright green, scroll follows your pace smoothly

**Test B: Off-script detection**
- Start reading, then say something completely off-script (e.g., "let me think about this...")
- Wait 4-5 seconds while still off-script
- EXPECTED: Waveform dims, scroll slows and stops, text stays visible (last spoken line not scrolled away)

**Test C: Return to script**
- After going off-script, resume reading from where you were (or from a new position)
- EXPECTED: Waveform brightens, scroll resumes smoothly (not a sudden jump)

**Test D: Skip ahead**
- Start reading at the beginning
- Skip ahead to a paragraph much further in the script
- EXPECTED: If you speak clearly (high confidence), scroll jumps to new position

**Test E: No dimming**
- As you read, check the text you've already passed
- EXPECTED: Previously read text is at full brightness (not dimmed to 50%)

**Test F: Smooth speed changes**
- Read at different paces
- EXPECTED: Scroll speed adjusts smoothly, no sudden jerks

Rate each test: PASS / PARTIAL / FAIL

Notes for any failures (what actually happened):
  </how-to-verify>
  <resume-signal>
Type "approved" if all tests pass, or describe issues found for each failing test
  </resume-signal>
</task>

</tasks>

<verification>
1. Voice mode activates and processes transcripts through confidence system
2. Console shows confidence levels (high/medium/low) and scroll states
3. AudioVisualizer brightness changes with confidence
4. Scroll stops before scrolling past last matched position
5. Human verification confirms all 6 test scenarios
</verification>

<success_criteria>
- All Phase 4 requirements (TRACK-02, TRACK-03, TRACK-04, TRACK-05) are met:
  - TRACK-02: Scrolls when confident in match
  - TRACK-03: Pauses when confidence is low (off-script)
  - TRACK-04: Detects and handles skips
  - TRACK-05: Visual confidence indicator in waveform
- User confirms smooth, human-operator-like scroll behavior
- No regression in basic teleprompter functionality
</success_criteria>

<output>
After completion, create `.planning/phases/04-intelligent-scroll-control/04-04-SUMMARY.md`
</output>
