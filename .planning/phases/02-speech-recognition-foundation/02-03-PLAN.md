---
phase: 02-speech-recognition-foundation
plan: 03
type: execute
wave: 2
depends_on: [02-01, 02-02]
files_modified:
  - script.js
autonomous: false

must_haves:
  truths:
    - "User grants microphone permission and app begins listening"
    - "User can see visual indicator showing when app is actively listening"
    - "App continues listening through pauses without manual restart"
    - "When speech recognition fails, app shows clear error and falls back to manual mode"
  artifacts:
    - path: "script.js"
      provides: "Voice mode integration and state management"
      contains: "voiceEnabled"
  key_links:
    - from: "script.js"
      to: "voice/SpeechRecognizer.js"
      via: "import and instantiation"
      pattern: "SpeechRecognizer"
    - from: "script.js"
      to: "voice/AudioVisualizer.js"
      via: "import and instantiation"
      pattern: "AudioVisualizer"
    - from: "script.js"
      to: "localStorage"
      via: "save/load voiceEnabled preference"
      pattern: "voiceEnabled"
---

<objective>
Wire together the SpeechRecognizer and AudioVisualizer modules with the main application, implementing the complete voice mode flow: toggle, permission, recognition, visualization, persistence, and error handling.

Purpose: This plan connects the individual modules into a cohesive voice mode experience. Users will be able to enable voice mode, see the listening indicator, and have their preference remembered across sessions.

Output: Updated script.js with full voice mode integration, ready for human verification.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-speech-recognition-foundation/02-CONTEXT.md
@.planning/phases/02-speech-recognition-foundation/02-RESEARCH.md
@.planning/phases/02-speech-recognition-foundation/02-01-SUMMARY.md
@.planning/phases/02-speech-recognition-foundation/02-02-SUMMARY.md
@script.js
@voice/SpeechRecognizer.js
@voice/AudioVisualizer.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Import modules and add voice state</name>
  <files>script.js</files>
  <action>
Update script.js to import the voice modules and add voice-related state.

At the top of script.js, add module imports. Since this is vanilla JS without a bundler, use dynamic imports or script tags. For simplicity, we'll use script tags in HTML and access globals.

First, update index.html to load the voice modules before script.js:
```html
<script src="voice/SpeechRecognizer.js"></script>
<script src="voice/AudioVisualizer.js"></script>
<script src="script.js" defer></script>
```

In script.js, add voice-related state to the initial state object:

```javascript
const { state, subscribe } = createState({
  mode: 'editor',
  fontSize: 48,
  scrollSpeed: 50,
  isScrolling: false,
  voiceEnabled: false,      // NEW: voice mode on/off
  voiceState: 'idle'        // NEW: 'idle' | 'listening' | 'error' | 'retrying'
});
```

Add module-level variables for voice components (similar to DOM elements):

```javascript
// Voice recognition components
let speechRecognizer = null;
let audioVisualizer = null;
let audioStream = null;
```

Add DOM element references in the DOMContentLoaded section:

```javascript
let voiceToggle;
let listeningIndicator;
let waveformCanvas;
```

And get them after DOMContentLoaded:
```javascript
voiceToggle = document.getElementById('voice-toggle');
listeningIndicator = document.getElementById('listening-indicator');
waveformCanvas = document.getElementById('waveform-canvas');
```

Check browser support and disable toggle if not supported:
```javascript
if (!SpeechRecognizer.isSupported()) {
  voiceToggle.disabled = true;
  voiceToggle.title = 'Voice recognition not supported in this browser. Use Chrome or Safari.';
}
```
  </action>
  <verify>
1. Load page - no console errors
2. Check that voiceEnabled is in state
3. In Firefox, voice toggle should be disabled with tooltip
4. In Chrome, voice toggle should be enabled
  </verify>
  <done>
- Voice modules loaded via script tags
- State includes voiceEnabled and voiceState
- Module-level variables declared for voice components
- DOM references obtained on load
- Browser support check disables toggle in unsupported browsers
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement voice toggle flow with permission handling</name>
  <files>script.js</files>
  <action>
Implement the complete voice toggle flow: permission request, recognition start, visualization start, and error handling.

Create the enableVoiceMode function:

```javascript
async function enableVoiceMode() {
  // Request microphone permission
  try {
    audioStream = await navigator.mediaDevices.getUserMedia({
      audio: {
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true
      }
    });
  } catch (err) {
    handleMicrophoneError(err);
    return;
  }

  // Initialize visualizer with the stream
  if (!audioVisualizer) {
    audioVisualizer = new AudioVisualizer(waveformCanvas);
  }
  audioVisualizer.start(audioStream);

  // Initialize and start speech recognizer
  if (!speechRecognizer) {
    speechRecognizer = new SpeechRecognizer({
      onTranscript: (text, isFinal) => {
        // For Phase 2, just log transcripts (Phase 3 will use them for matching)
        console.log(`[Voice] ${isFinal ? 'FINAL' : 'interim'}: ${text}`);
      },
      onError: (errorType, isFatal) => {
        console.error(`[Voice] Error: ${errorType} (fatal: ${isFatal})`);
        if (isFatal) {
          disableVoiceMode();
          showVoiceError(`Voice recognition error: ${errorType}`);
        }
      },
      onStateChange: (newState) => {
        state.voiceState = newState;
        updateVoiceIndicator();
      }
    });
  }

  speechRecognizer.start();

  // Update UI
  state.voiceEnabled = true;
  voiceToggle.classList.add('active');
  listeningIndicator.classList.remove('hidden');
}
```

Create the disableVoiceMode function:

```javascript
function disableVoiceMode() {
  // Stop speech recognition
  if (speechRecognizer) {
    speechRecognizer.stop();
  }

  // Stop visualizer (this also stops the audio stream)
  if (audioVisualizer) {
    audioVisualizer.stop();
  }

  // Clear references
  audioStream = null;

  // Update UI
  state.voiceEnabled = false;
  state.voiceState = 'idle';
  voiceToggle.classList.remove('active');
  listeningIndicator.classList.add('hidden');
}
```

Create error handling functions:

```javascript
function handleMicrophoneError(err) {
  let message = 'Unable to access microphone.';

  switch (err.name) {
    case 'NotAllowedError':
      message = 'Microphone permission denied. Voice mode requires microphone access.';
      voiceToggle.disabled = true;
      voiceToggle.title = message;
      break;
    case 'NotFoundError':
      message = 'No microphone detected.';
      break;
    case 'NotReadableError':
      message = 'Microphone is in use by another application.';
      break;
  }

  console.error('[Voice] Microphone error:', err.name, message);
  showVoiceError(message);
}

function showVoiceError(message) {
  // Brief error display - could be enhanced with toast notification
  // For now, use alert as simple fallback
  alert(message);
}
```

Create the toggle handler and attach to button:

```javascript
function toggleVoiceMode() {
  if (state.voiceEnabled) {
    disableVoiceMode();
  } else {
    enableVoiceMode();
  }
}

// In DOMContentLoaded, add:
voiceToggle.addEventListener('click', toggleVoiceMode);
```
  </action>
  <verify>
1. Click Voice button - permission prompt appears
2. Grant permission - listening indicator shows, button turns green
3. Speak - check console for transcript logs
4. Click Voice button again - indicator hides, button returns to normal
5. Reload page, deny permission - error message appears, button disabled
  </verify>
  <done>
- Voice toggle enables/disables voice mode
- Microphone permission requested on first enable
- Visualizer shows when listening
- Recognizer logs transcripts to console
- Errors handled gracefully with user feedback
  </done>
</task>

<task type="auto">
  <name>Task 3: Add persistence and indicator state updates</name>
  <files>script.js</files>
  <action>
Add localStorage persistence for voice preference and visual state updates for retrying/error states.

Update the settings persistence functions:

```javascript
function saveSettings() {
  const settings = {
    scrollSpeed: state.scrollSpeed,
    fontSize: state.fontSize,
    voiceEnabled: state.voiceEnabled   // NEW
  };
  localStorage.setItem(SETTINGS_KEY, JSON.stringify(settings));
}

function loadSettings() {
  try {
    const saved = localStorage.getItem(SETTINGS_KEY);
    if (saved) {
      const settings = JSON.parse(saved);
      state.scrollSpeed = settings.scrollSpeed ?? 50;
      state.fontSize = settings.fontSize ?? 48;
      state.voiceEnabled = settings.voiceEnabled ?? false;  // NEW
    }
  } catch (e) {
    console.error('Failed to load settings:', e);
  }
}
```

Create the indicator update function:

```javascript
function updateVoiceIndicator() {
  if (!audioVisualizer) return;

  switch (state.voiceState) {
    case 'listening':
      audioVisualizer.setErrorState(false);  // Green bars
      break;
    case 'retrying':
    case 'error':
      audioVisualizer.setErrorState(true);   // Amber bars
      break;
    case 'idle':
      // No change needed, indicator hidden when idle
      break;
  }
}
```

Update the subscribe handler to save voiceEnabled:

```javascript
subscribe((property, value) => {
  // ... existing handlers ...

  // Auto-save settings (add voiceEnabled)
  if (['scrollSpeed', 'fontSize', 'voiceEnabled'].includes(property)) {
    saveSettings();
  }
});
```

Add auto-restore of voice mode on page load (in DOMContentLoaded, after loadSettings):

```javascript
// Auto-restore voice mode if it was enabled
if (state.voiceEnabled && SpeechRecognizer.isSupported()) {
  // Only restore in teleprompter mode, and when entering teleprompter
  // For now, just reset - we'll enable when entering teleprompter
  state.voiceEnabled = false;
}
```

Actually, better approach: restore voice mode when entering teleprompter mode if it was previously enabled. Update switchMode:

```javascript
function switchMode(newMode) {
  if (newMode === 'teleprompter') {
    // ... existing code ...

    // Restore voice mode if previously enabled
    const savedVoice = loadSettings().voiceEnabled;
    if (savedVoice && SpeechRecognizer.isSupported() && !state.voiceEnabled) {
      enableVoiceMode();
    }
  } else if (newMode === 'editor') {
    // Stop voice mode when exiting teleprompter
    if (state.voiceEnabled) {
      disableVoiceMode();
    }

    // ... existing code ...
  }
}
```

Wait, loadSettings modifies state directly. Let's use localStorage directly:

```javascript
// In switchMode('teleprompter'):
const savedSettings = JSON.parse(localStorage.getItem(SETTINGS_KEY) || '{}');
if (savedSettings.voiceEnabled && SpeechRecognizer.isSupported() && !state.voiceEnabled) {
  // Small delay to let UI settle
  setTimeout(() => enableVoiceMode(), 100);
}
```
  </action>
  <verify>
1. Enable voice mode, close page
2. Reopen, enter teleprompter mode
3. Voice mode should auto-enable
4. Exit to editor, voice mode should stop
5. Re-enter teleprompter, voice mode restores
6. Disable voice mode manually, exit and re-enter - should stay disabled
  </verify>
  <done>
- Voice preference persists in localStorage
- Voice mode auto-restores when entering teleprompter (if was enabled)
- Voice mode stops when exiting to editor
- Indicator color changes based on voiceState (green normal, amber retrying)
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete voice mode integration with:
- Voice toggle button in teleprompter controls
- Microphone permission flow
- Speech recognition with auto-restart
- Animated waveform indicator showing listening state
- Amber indicator during retry attempts
- localStorage persistence of voice preference
  </what-built>
  <how-to-verify>
Test in Chrome (Firefox does not support Web Speech API):

1. **Basic flow:**
   - Open the app and paste some text
   - Click "Start Teleprompter"
   - Click "Voice" button
   - Grant microphone permission when prompted
   - Verify: Waveform indicator appears in top-right corner
   - Verify: Voice button turns green
   - Speak into microphone
   - Verify: Waveform bars animate in response to your voice
   - Check console (F12) for transcript logs

2. **Auto-restart (silence test):**
   - With voice mode enabled, stay silent for 10+ seconds
   - Verify: Recognition continues (no manual restart needed)
   - Speak again
   - Verify: Transcripts still appearing in console

3. **Error recovery:**
   - Disconnect from internet briefly (if possible)
   - Verify: Indicator turns amber during retry
   - Reconnect
   - Verify: Indicator returns to green when recognition resumes

4. **Toggle and persistence:**
   - Click Voice button to disable
   - Verify: Indicator disappears, button returns to normal
   - Exit to editor, then re-enter teleprompter
   - Verify: Voice mode stays disabled (not auto-enabled)
   - Enable voice mode again
   - Refresh the page
   - Enter teleprompter mode
   - Verify: Voice mode auto-enables (setting persisted)

5. **Fullscreen:**
   - Enable voice mode
   - Press F for fullscreen
   - Verify: Waveform indicator still visible in top-right corner

6. **Permission denial (fresh browser profile or incognito):**
   - In a fresh incognito window, open the app
   - Enter teleprompter mode
   - Click Voice button
   - Deny microphone permission
   - Verify: Error message appears
   - Verify: Voice button becomes disabled with tooltip
  </how-to-verify>
  <resume-signal>Type "approved" to confirm Phase 2 requirements met, or describe any issues found</resume-signal>
</task>

</tasks>

<verification>
Phase 2 requirements verification:
- VOICE-01: User grants microphone permission and app begins listening [Click Voice -> Grant -> Listening]
- VOICE-02: User can see visual indicator showing when app is actively listening [Waveform in corner]
- VOICE-03: App continues listening through pauses without manual restart [Silence test]
- VOICE-04: App recovers gracefully from errors with manual fallback [Permission denial -> disabled toggle]
</verification>

<success_criteria>
All four VOICE-* requirements from REQUIREMENTS.md are satisfied:
1. Microphone permission flow works correctly
2. Waveform indicator provides clear visual feedback
3. Auto-restart handles Chrome's session timeouts
4. Fatal errors disable voice mode and provide user feedback
</success_criteria>

<output>
After completion, create `.planning/phases/02-speech-recognition-foundation/02-03-SUMMARY.md`
</output>
