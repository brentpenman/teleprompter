---
phase: 10-voskrecognizer-adapter
plan: 03
type: execute
wave: 3
depends_on: [10-02]
files_modified:
  - voice/VoskRecognizer.js
autonomous: true

must_haves:
  truths:
    - "Vosk 'partialresult' events trigger onTranscript(text, false) callbacks"
    - "Vosk 'result' events trigger onTranscript(text, true) callbacks"
    - "Existing components (WordMatcher, PositionTracker, ScrollController) work unchanged with VoskRecognizer"
    - "AudioVisualizer can connect to VoskRecognizer's audio stream"
  artifacts:
    - path: "voice/VoskRecognizer.js"
      provides: "Complete event mapping from Vosk to SpeechRecognizer callbacks"
      contains: "on('result')"
      min_lines: 220
  key_links:
    - from: "voice/VoskRecognizer.js"
      to: "recognizer.on('result')"
      via: "Final transcript callback"
      pattern: "on\\('result'"
    - from: "voice/VoskRecognizer.js"
      to: "recognizer.on('partialresult')"
      via: "Interim transcript callback"
      pattern: "on\\('partialresult'"
    - from: "voice/VoskRecognizer.js"
      to: "options.onTranscript"
      via: "Callback invocation"
      pattern: "onTranscript\\?"
---

<objective>
Map Vosk's event-based API ('result', 'partialresult') to SpeechRecognizer's callback API (onTranscript with isFinal flag). This completes the adapter pattern and enables existing components to work unchanged with VoskRecognizer.

Purpose: Achieve seamless integration with existing pipeline (WordMatcher, PositionTracker, ScrollController, AudioVisualizer) without any modifications to those components. Fulfill requirements for interim results (VOSK-04), final results (VOSK-05), and downstream compatibility (INTEG-01, INTEG-02).

Output: Complete, production-ready VoskRecognizer that can be dropped in as a SpeechRecognizer replacement.
</objective>

<execution_context>
@/Users/brent/.claude/get-shit-done/workflows/execute-plan.md
@/Users/brent/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/10-voskrecognizer-adapter/10-RESEARCH.md
@.planning/phases/10-voskrecognizer-adapter/10-01-SUMMARY.md
@.planning/phases/10-voskrecognizer-adapter/10-02-SUMMARY.md
@voice/VoskRecognizer.js
@voice/SpeechRecognizer.js
</context>

<tasks>

<task type="auto">
  <name>Map Vosk events to onTranscript callbacks</name>
  <files>voice/VoskRecognizer.js</files>
  <action>
Update VoskRecognizer.js start() method to set up Vosk event listeners right after creating KaldiRecognizer:

Add this code after `this._recognizer = new this._model.KaldiRecognizer(this._sampleRate);`:

```javascript
// Map Vosk events to SpeechRecognizer callback interface (from RESEARCH.md Pattern 5)

// Partial results (interim transcription) -> onTranscript(text, false)
this._recognizer.on('partialresult', (message) => {
  const text = message.result.partial;
  if (text && text.trim()) {
    this._options.onTranscript?.(text, false); // isFinal = false (VOSK-04)
  }
});

// Final results (utterance complete) -> onTranscript(text, true)
this._recognizer.on('result', (message) => {
  const text = message.result.text;
  if (text && text.trim()) {
    this._options.onTranscript?.(text, true); // isFinal = true (VOSK-05)
  }
});
```

Key points from research:
1. Vosk emits 'partialresult' events during speech (interim transcription)
2. Vosk emits 'result' events when utterance is complete (final transcription)
3. Both should be filtered for empty/whitespace-only text (avoid spamming callbacks)
4. isFinal flag is critical for WordMatcher/PositionTracker to distinguish confirmed vs tentative matches
5. No need for error event handlers - Vosk errors thrown during acceptWaveform, handled in try-catch

Difference from Web Speech API:
- Web Speech: Single 'result' event with array of alternatives, isFinal property on result object
- Vosk: Separate 'result' and 'partialresult' events, adapter maps to isFinal true/false
  </action>
  <verify>
1. Check VoskRecognizer.js has recognizer.on('partialresult') listener
2. Verify listener calls onTranscript(text, false) for interim results
3. Check VoskRecognizer.js has recognizer.on('result') listener
4. Verify listener calls onTranscript(text, true) for final results
5. Verify both listeners filter out empty/whitespace text
  </verify>
  <done>Vosk events mapped to onTranscript callbacks with correct isFinal flags, matching SpeechRecognizer interface</done>
</task>

<task type="auto">
  <name>Add getAudioContext() method for AudioVisualizer compatibility</name>
  <files>voice/VoskRecognizer.js</files>
  <action>
Add getAudioContext() method to VoskRecognizer class to enable AudioVisualizer integration (INTEG-02 requirement):

AudioVisualizer needs access to the AudioContext to create an AnalyserNode for waveform visualization. SpeechRecognizer doesn't expose this because Web Speech API doesn't provide access to audio stream, but VoskRecognizer has direct access to AudioContext.

Add this method after isListening() and isPaused():

```javascript
/**
 * Get the AudioContext for audio visualization
 * @returns {AudioContext|null} The AudioContext if listening, null otherwise
 */
getAudioContext() {
  return this._audioContext;
}

/**
 * Get the MediaStreamSource for audio visualization
 * @returns {MediaStreamAudioSourceNode|null} The source if listening, null otherwise
 */
getMediaStreamSource() {
  return this._source;
}
```

This enables AudioVisualizer to connect like:
```javascript
// In AudioVisualizer.js (future integration)
const audioContext = recognizer.getAudioContext();
const source = recognizer.getMediaStreamSource();
if (audioContext && source) {
  const analyser = audioContext.createAnalyser();
  source.connect(analyser);
  // ... visualization logic
}
```

Note: SpeechRecognizer doesn't have this method (Web Speech API limitation), but this is acceptable - AudioVisualizer will detect which recognizer is in use and adapt accordingly. This is handled in Phase 11 integration.
  </action>
  <verify>
1. Check VoskRecognizer.js has getAudioContext() method
2. Verify it returns this._audioContext
3. Check VoskRecognizer.js has getMediaStreamSource() method
4. Verify it returns this._source
5. Verify both return null when not listening
  </verify>
  <done>VoskRecognizer exposes audio context and source for AudioVisualizer integration</done>
</task>

<task type="auto">
  <name>Add comprehensive JSDoc documentation and error handling</name>
  <files>voice/VoskRecognizer.js</files>
  <action>
Add JSDoc comments and error handling to match SpeechRecognizer's quality:

1. Add file-level JSDoc comment at top:
```javascript
/**
 * VoskRecognizer - Vosk offline speech recognition adapter
 *
 * Implements identical interface to SpeechRecognizer, enabling drop-in replacement
 * of Web Speech API with Vosk offline recognition.
 *
 * Key differences from SpeechRecognizer:
 * - Requires model to be loaded via loadModel() before start()
 * - No auto-restart logic (Vosk runs continuously)
 * - No retry/backoff (offline, no network errors)
 * - Requires cross-origin isolation (SharedArrayBuffer for WASM)
 * - Supports getAudioContext() for AudioVisualizer integration
 *
 * @example
 * if (!VoskRecognizer.isSupported()) {
 *   console.log('Vosk not supported (requires SharedArrayBuffer)');
 *   return;
 * }
 *
 * const recognizer = new VoskRecognizer({
 *   onTranscript: (text, isFinal) => console.log(text, isFinal),
 *   onError: (errorType, isFatal) => console.error(errorType, isFatal),
 *   onStateChange: (state) => console.log('State:', state)
 * });
 *
 * // Load model (from ModelLoader)
 * await recognizer.loadModel(modelArrayBuffer);
 *
 * // Start recognition
 * await recognizer.start();
 *
 * // ... later
 * await recognizer.stop();
 */
```

2. Add JSDoc to loadModel() method:
```javascript
/**
 * Load Vosk model from ArrayBuffer
 * Must be called before start(). Model creation spawns Web Worker (~1-2s).
 * @param {ArrayBuffer} modelArrayBuffer - Vosk model binary data
 * @returns {Promise<void>}
 * @throws {Error} If model creation fails
 */
async loadModel(modelArrayBuffer) {
  // ... existing implementation
}
```

3. Add JSDoc to start() method:
```javascript
/**
 * Start speech recognition
 * Triggers browser's mic permission prompt if not already granted.
 * @returns {Promise<void>}
 * @throws {Error} If model not loaded (call loadModel() first)
 * @throws {DOMException} If microphone permission denied (NotAllowedError)
 */
async start() {
  // ... existing implementation
}
```

4. Add JSDoc to other public methods (stop, pause, resume) matching SpeechRecognizer style

5. Add error handling improvements:
   - Wrap recognizer.on() calls in try-catch in case Vosk throws during event setup
   - Add console.warn for ScriptProcessor deprecation (from RESEARCH.md Pitfall 2):
     ```javascript
     // After creating ScriptProcessor
     // Note: ScriptProcessor is deprecated but still functional
     // vosk-browser doesn't support AudioWorklet yet (migration planned for v2)
     ```

6. Export VoskRecognizer as default (matching SpeechRecognizer)
  </action>
  <verify>
1. Check file has comprehensive header JSDoc
2. Verify all public methods have JSDoc comments
3. Verify JSDoc matches SpeechRecognizer's style and detail level
4. Check error messages are developer-friendly
5. Verify ScriptProcessor deprecation noted in comment
  </verify>
  <done>VoskRecognizer has complete documentation and error handling matching SpeechRecognizer quality</done>
</task>

</tasks>

<verification>
1. Code review: Compare VoskRecognizer.js side-by-side with SpeechRecognizer.js
   - Same method names? ✓
   - Same callback signatures? ✓
   - Similar error handling? ✓
   - Similar documentation quality? ✓

2. Integration test: Create simple test that uses VoskRecognizer with existing components
   ```javascript
   // In script.js or test file
   const recognizer = new VoskRecognizer({
     onTranscript: handleSpeechTranscript, // Existing handler from v1.1
     onError: (errorType, isFatal) => console.error(errorType, isFatal),
     onStateChange: (state) => console.log('State:', state)
   });
   ```

3. Check callback signatures match:
   - onTranscript receives (text: string, isFinal: boolean)
   - onError receives (errorType: string, isFatal: boolean)
   - onStateChange receives (state: 'idle' | 'listening' | 'error')

4. Verify event mapping works:
   - Vosk 'partialresult' -> onTranscript(text, false)
   - Vosk 'result' -> onTranscript(text, true)

5. Check INTEG-01 requirement: Existing components should work unchanged
   - WordMatcher.match() receives same transcript format
   - PositionTracker.update() receives same match results
   - ScrollController.update() receives same position updates
</verification>

<success_criteria>
- Vosk 'partialresult' events mapped to onTranscript(text, false)
- Vosk 'result' events mapped to onTranscript(text, true)
- Empty/whitespace-only transcripts filtered out
- getAudioContext() and getMediaStreamSource() methods added for AudioVisualizer
- Comprehensive JSDoc documentation matching SpeechRecognizer quality
- VoskRecognizer can be used as drop-in replacement for SpeechRecognizer
- VOSK-04, VOSK-05, INTEG-01, INTEG-02 requirements satisfied
- All 10 VOSK requirements (VOSK-01 through VOSK-10) now satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/10-voskrecognizer-adapter/10-03-SUMMARY.md`
</output>
